{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoNFkT3rgjre"
      },
      "source": [
        "## **Try Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Rhozrjn0gm"
      },
      "source": [
        "# Python Script to Extract tweets of a\n",
        "# particular Hashtag using Tweepy and Pandas\n",
        "\n",
        "\n",
        "# import modules\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "\n",
        "\n",
        "# function to display data of each tweet\n",
        "def printtweetdata(n, ith_tweet):\n",
        "\tprint()\n",
        "\tprint(f\"Tweet {n}:\")\n",
        "\tprint(f\"Username:{ith_tweet[0]}\")\n",
        "\tprint(f\"Description:{ith_tweet[1]}\")\n",
        "\tprint(f\"Location:{ith_tweet[2]}\")\n",
        "\tprint(f\"Following Count:{ith_tweet[3]}\")\n",
        "\tprint(f\"Follower Count:{ith_tweet[4]}\")\n",
        "\tprint(f\"Total Tweets:{ith_tweet[5]}\")\n",
        "\tprint(f\"Retweet Count:{ith_tweet[6]}\")\n",
        "\tprint(f\"Tweet Text:{ith_tweet[7]}\")\n",
        "\tprint(f\"Hashtags Used:{ith_tweet[8]}\")\n",
        "\n",
        "\n",
        "# function to perform data extraction\n",
        "def scrape(words, date_since, numtweet):\n",
        "\t\n",
        "\t# Creating DataFrame using pandas\n",
        "\tdb = pd.DataFrame(columns=['username', 'description', 'location', 'following',\n",
        "\t\t\t\t\t\t\t'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags'])\n",
        "\t\n",
        "\t# We are using .Cursor() to search through twitter for the required tweets.\n",
        "\t# The number of tweets can be restricted using .items(number of tweets)\n",
        "\ttweets = tweepy.Cursor(api.search, q=words, lang=\"en\",\n",
        "\t\t\t\t\t\tsince=date_since, tweet_mode='extended').items(numtweet)\n",
        "\t\n",
        "\t# .Cursor() returns an iterable object. Each item in\n",
        "\t# the iterator has various attributes that you can access to\n",
        "\t# get information about each tweet\n",
        "\tlist_tweets = [tweet for tweet in tweets]\n",
        "\t\n",
        "\t# Counter to maintain Tweet Count\n",
        "\ti = 1\n",
        "\t\n",
        "\t# we will iterate over each tweet in the list for extracting information about each tweet\n",
        "\tfor tweet in list_tweets:\n",
        "\t\tusername = tweet.user.screen_name\n",
        "\t\tdescription = tweet.user.description\n",
        "\t\tlocation = tweet.user.location\n",
        "\t\tfollowing = tweet.user.friends_count\n",
        "\t\tfollowers = tweet.user.followers_count\n",
        "\t\ttotaltweets = tweet.user.statuses_count\n",
        "\t\tretweetcount = tweet.retweet_count\n",
        "\t\thashtags = tweet.entities['hashtags']\n",
        "\t\t\n",
        "\t\t# Retweets can be distinguished by a retweeted_status attribute,\n",
        "\t\t# in case it is an invalid reference, except block will be executed\n",
        "\t\ttry:\n",
        "\t\t\ttext = tweet.retweeted_status.full_text\n",
        "\t\texcept AttributeError:\n",
        "\t\t\ttext = tweet.full_text\n",
        "\t\thashtext = list()\n",
        "\t\tfor j in range(0, len(hashtags)):\n",
        "\t\t\thashtext.append(hashtags[j]['text'])\n",
        "\t\t\n",
        "\t\t# Here we are appending all the extracted information in the DataFrame\n",
        "\t\tith_tweet = [username, description, location, following,\n",
        "\t\t\t\t\tfollowers, totaltweets, retweetcount, text, hashtext]\n",
        "\t\tdb.loc[len(db)] = ith_tweet\n",
        "\t\t\n",
        "\t\t# Function call to print tweet data on screen\n",
        "\t\tprinttweetdata(i, ith_tweet)\n",
        "\t\ti = i+1\n",
        "\tfilename = 'scraped_tweets.csv'\n",
        "\t\n",
        "\t# we will save our database as a CSV file.\n",
        "\tdb.to_csv(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3nAYFOjn4fz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "90490f92-614e-47e0-f22b-3e6a62b7c287"
      },
      "source": [
        "consumer_key = \"bBSZnia3f9WCoKvgSaEauSzZi\"\n",
        "consumer_secret = \"fafiDLcGll6C2wkPc3DnupmhbIDB7JzOljq7WKs9JdE8DKbqNo\"\n",
        "access_key = \"1455935249980923909-b7kJY3CPYomM0lpkcYmaS1S3LKOSpD\"\n",
        "access_secret = \"BtldEZimd7NxfJExYyS3kK2QxlzFPtpoosqyjhTB9hI9l\"\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Enter Hashtag and initial date\n",
        "print(\"Enter Twitter HashTag to search for\")\n",
        "words = input()\n",
        "print(\"Enter Date since The Tweets are required in yyyy-mm-dd\")\n",
        "date_since = input()\n",
        "\n",
        "# number of tweets you want to extract in one run\n",
        "numtweet = 10000\n",
        "scrape(words, date_since, numtweet)\n",
        "print('Scraping has completed!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Twitter HashTag to search for\n",
            "#bjp\n",
            "Enter Date since The Tweets are required in yyyy-mm-dd\n",
            "2010-01-01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TweepError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9aa24f8b46af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# number of tweets you want to extract in one run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnumtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_since\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraping has completed!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2ddd2dffb020>\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(words, date_since, numtweet)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# the iterator has various attributes that you can access to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# get information about each tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlist_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Counter to maintain Tweet Count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2ddd2dffb020>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# the iterator has various attributes that you can access to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# get information about each tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlist_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Counter to maintain Tweet Count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTweepError\u001b[0m: Twitter error response: status code = 429"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSGfm7U6mwiq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nERnpUsH1hwd"
      },
      "source": [
        "# **Python Script to Extract tweets of a particular Hashtag using Tweepy and Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW7xez8Cmwfw"
      },
      "source": [
        "# import modules\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "\n",
        "\n",
        "# function to display data of each tweet\n",
        "def printtweetdata(n, ith_tweet):\n",
        "    print()\n",
        "    print(f\"Tweet {n}:\")\n",
        "    print(f\"Username:{ith_tweet[0]}\")\n",
        "    print(f\"Person Name:{ith_tweet[13]}\")\n",
        "    print(f\"Description:{ith_tweet[1]}\")\n",
        "    print(f\"Location:{ith_tweet[2]}\")\n",
        "    print(f\"Following Count:{ith_tweet[3]}\")\n",
        "    print(f\"Follower Count:{ith_tweet[4]}\")\n",
        "    print(f\"Total Tweets:{ith_tweet[5]}\")\n",
        "    print(f\"Retweet Count:{ith_tweet[6]}\")\n",
        "    print(f\"Tweet Text:{ith_tweet[7]}\")\n",
        "    print(f\"Hashtags Used:{ith_tweet[8]}\")\n",
        "    print(f\"Date:{ith_tweet[9]}\")\n",
        "    print(f\"Tweet ID:{ith_tweet[10]}\")\n",
        "    print(f\"lat:{ith_tweet[11]}\")\n",
        "    print(f\"lng:{ith_tweet[12]}\")\n",
        "\n",
        "\n",
        "\n",
        "# function to perform data extraction\n",
        "def scrape(words, date_since, numtweet):\n",
        "\t\n",
        "    # Creating DataFrame using pandas\n",
        "    db = pd.DataFrame(columns=['username', 'description', 'location', 'following',\n",
        "                'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags', 'date', 'tweetID', 'lat', 'lng', 'nameUser'])\n",
        "    # for tweet in tweepy.Cursor(api.search,geocode=geoc).items(1000):\n",
        "    tweet_lst=[]\n",
        "    i = 1\n",
        "    geoc = '28.7041, 77.1025, 1000mi'\n",
        "    # Washington --> '38.9072,-77.0369,1000mi' \n",
        "    #  Delhi --> '28.7041, 77.1025, 1000mi'\n",
        "    for tweet in tweepy.Cursor(api.search, geocode=geoc, q=words, lang=\"en\", since=date_since, tweet_mode='extended').items(numtweet):\n",
        "      if(tweet.coordinates !=None):\n",
        "        username = tweet.user.screen_name\n",
        "        name = tweet.user.name\n",
        "        tweetDate = tweet.created_at.date()\n",
        "        description = tweet.user.description\n",
        "        location = tweet.user.location\n",
        "        following = tweet.user.friends_count\n",
        "        followers = tweet.user.followers_count\n",
        "        totaltweets = tweet.user.statuses_count\n",
        "        retweetcount = tweet.retweet_count\n",
        "        # text = tweet.text\n",
        "        try:\n",
        "          text = tweet.retweeted_status.full_text\n",
        "        except AttributeError:\n",
        "          text = tweet.full_text\n",
        "        hashtags = tweet.entities['hashtags']\n",
        "        hashtext = list()\n",
        "        for j in range(0, len(hashtags)):\n",
        "          hashtext.append(hashtags[j]['text'])\n",
        "\n",
        "        # Here we are appending all the extracted information in the DataFrame\n",
        "        ith_tweet = [username, description, location, following, followers, totaltweets, retweetcount, text, \n",
        "                  hashtext, tweetDate, tweet.id, tweet.coordinates['coordinates'][0], tweet.coordinates['coordinates'][1], name]\n",
        "        db.loc[len(db)] = ith_tweet\n",
        "\n",
        "        # Function call to print tweet data on screen\n",
        "        printtweetdata(i, ith_tweet)\n",
        "        i = i+1\n",
        "\n",
        "    filename = 'scraped_tweets.csv'\n",
        "    \n",
        "    # we will save our database as a CSV file.\n",
        "    db.to_csv(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuIBocPdmwcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf09188-9841-4a9e-a42d-b793077c09c9"
      },
      "source": [
        "consumer_key = \"bBSZnia3f9WCoKvgSaEauSzZi\"\n",
        "consumer_secret = \"fafiDLcGll6C2wkPc3DnupmhbIDB7JzOljq7WKs9JdE8DKbqNo\"\n",
        "access_key = \"1455935249980923909-b7kJY3CPYomM0lpkcYmaS1S3LKOSpD\"\n",
        "access_secret = \"BtldEZimd7NxfJExYyS3kK2QxlzFPtpoosqyjhTB9hI9l\"\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication OK\")\n",
        "except:\n",
        "    print(\"Error during authentication\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnUoyB1ImwZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e36afca-26b8-4bd1-b417-563dcda2ef87"
      },
      "source": [
        "# Enter Hashtag and initial date\n",
        "print(\"Enter Twitter HashTag to search for\")\n",
        "words = input()\n",
        "print(\"Enter Date since The Tweets are required in yyyy-mm-dd\")\n",
        "date_since = input()\n",
        "\n",
        "# number of tweets you want to extract in one run\n",
        "numtweet = 10000\n",
        "scrape(words, date_since, numtweet)\n",
        "print('Scraping has completed!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Twitter HashTag to search for\n",
            "#bjp\n",
            "Enter Date since The Tweets are required in yyyy-mm-dd\n",
            "2010-01-01\n",
            "Scraping has completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye2qf2WBmwVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "232229ae-17fc-4008-e463-2b928c6e833b"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"scraped_tweets.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_160c1f00-36f3-4e55-86ab-d2a2b3f83287\", \"scraped_tweets.csv\", 12000)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq49gJbZmwSc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}